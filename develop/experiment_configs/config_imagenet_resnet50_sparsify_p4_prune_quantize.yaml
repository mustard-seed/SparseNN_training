# YAML file parsers treats scientific notaiton as strings
#Configuration for training baseline ResNet-56 on CIFAR 10
#See Section 4.2 of Deep Residual Learning for Image Classification for settings

# Iterations, etc
numEpochToTrain: 9
batchSizePerWorker: 32
numThreadsPerWorker: 28

# Learning rates

#From the Accurate, Large minibatch SGD
#'We start from a learning rate of eta, and increment it by a constant amount at each iteration
#such that it reaches eta_hat = k * eta after 5 epochs' (2.2, Gradual warmup)
#'eta = 0.1 * k * n / 256', where n is 32, and n is 8 (5.1)
#
#In the original ResNet-50 paper, the (effective) mini-batch size is 256, and the learning rate is 0.1
#So with a smaller batch size, which is 1/16 of theirs, the learning rate ought to be 0.00625
#DIFF:
# The longer the training takes on the cluster, the more the chance it will run into some kind of error
# So we increase the unattenuated per-worker learning rate to 0.0125 (2x)
# We cool the learning schedule faster than the original ImageNet.
# Sparsification:
#   - Prior to sparsification, the baseline's learning rate per-worker is 0.0000125
#   - We note that when the learning rate per worker was adjusted 0.00125, the top-1 accuracy was already more than 50%
#   - We speculate that the sparsification loss would drive the accuracy back to around 50% top-1 validation accuracy
#   - So set the initial learning rate 0.00125
# Pruning & quantization:
#   - Prior to pruning, the learning rate per worker is 0.0000125
#   - We speculate the pruning would perturb the model, and drive down the accuracy by about 5% initially
#   - So maybe set the inital learning rate to be 100x higher, 0.00125
#   - Keep this learning rate for 4 epochs
#   - Freeze BN stats and after 5 epochs
#   - Freeze QAT observer stats after 6 epochs
#   - Then attenuate it by a factor of 10
lrInitialBase: 0.00125
lrPeakBase: 0.00125
lrWarmupEpochs: 1
lrStepSize: 3
lrReductionFactor: 0.1
lrMomentum: 0.9
lrMPAdjustmentWarmUpEpochs: 1

# Loss contribution
enableLossActivation: True
enableLossWeightL1: True
enableLossWeightL2: True
lossActivationLambda: 0.0000005
lossWeightL1Lambda: 0.000005
lossWeightL2Lambda: 0.0001

# Pruning and quantization parameters
prune: True
pruneCluster: 4
pruneThreshold: 0.00001
quantize: True
numUpdateBNStatsEpochs: 4
numUpdateQatObserverEpochs: 5

# See
seed: 47

# Dataset location
dataTrainDir: '/homes/jmusel/jmuse/imagenet/train'
dataValDir: '/homes/jmusel/jmuse/imagenet/val'

# Checkpoints
checkpointSaveDir: 'experiment_logs/imagenet_resnet50_sparsify_p4_prune_quantize_log'
checkpointSaveFileNameFormat: 'ckpt_sparsify_p4_prune_quantize_epoch{}.pth.tar'

# Log Dir
logDir: 'experiment_logs/imagenet_resnet50_sparsify_p4_prune_quantize_log'