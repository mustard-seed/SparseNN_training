# YAML file parsers treats scientific notaiton as strings
#Configuration for training baseline ResNet-56 on CIFAR 10
#See Section 4.2 of Deep Residual Learning for Image Classification for settings

# Iterations, etc
numEpochToTrain: 15
batchSizePerWorker: 32
numThreadsPerWorker: 28

# Learning rates

#From the Accurate, Large minibatch SGD
#'We start from a learning rate of eta, and increment it by a constant amount at each iteration
#such that it reaches eta_hat = k * eta after 5 epochs' (2.2, Gradual warmup)
#'eta = 0.1 * k * n / 256', where n is 32, and n is 8 (5.1)
#
#In the original ResNet-50 paper, the (effective) mini-batch size is 256, and the learning rate is 0.1
#So with a smaller batch size, which is 1/16 of theirs, the learning rate ought to be 0.00625
#DIFF:
# The longer the training takes on the cluster, the more the chance it will run into some kind of error
# So we increase the unattenuated per-worker learning rate to 0.0125 (2x)
# We cool the learning schedule faster than the original ImageNet.
# Sparsification:
#   - Prior to sparsification, the baseline's learning rate per-worker is 0.0000125
#   - We note that when the learning rate per worker was adjusted 0.00125, the top-1 accuracy was already more than 50%
#   - We speculate that the sparsification loss would drive the accuracy back to around 50% top-1 validation accuracy
#   - So set the initial learning rate 0.00125
# Quantization
#    - Prior to quantization, the learning rate per worker is 0.0000125
#    - Keep the learning rate the same and enable quantization.
#    - Keep the sparsification losses
#    - Keep updating BN stats and QAT stats.
lrInitialBase: 0.0000125
lrPeakBase: 0.0000125
lrWarmupEpochs: 1
lrStepSize: 5
lrReductionFactor: 0.1
lrMomentum: 0.9
lrMPAdjustmentWarmUpEpochs: 3

# Loss contribution
enableLossActivation: True
enableLossWeightL1: True
enableLossWeightL2: True
lossActivationLambda: 0.0000005
lossWeightL1Lambda: 0.000005
lossWeightL2Lambda: 0.0001

# Pruning and quantization parameters
prune: False
pruneCluster: 1
pruneThreshold: 0.005
quantize: True
numUpdateBNStatsEpochs: 100
numUpdateQatObserverEpochs: 100

# See
seed: 47

# Dataset location
dataTrainDir: '/homes/jmusel/jmuse/imagenet/train'
dataValDir: '/homes/jmusel/jmuse/imagenet/val'

# Checkpoints
checkpointSaveDir: 'experiment_logs/imagenet_resnet50_sparsify_p1_quantize_log'
checkpointSaveFileNameFormat: 'ckpt_sparsify_p1_quantize.epoch{}.pth.tar'

# Log Dir
logDir: 'experiment_logs/imagenet_resnet50_sparsify_p1_quantize_log'